#+TITLE: Introduction into Operating Systems
#+DATE:  <2022-08-01>


#+begin_abstract
Some notes and exercises from OPERATING-SYSTEM CONCEPTS
#+end_abstract


An operating system is software that manages a computer’s hardware. It also provides a basis
for application programs and acts as an intermediary between the computer user and the
computer hardware. An amazing aspect of operating systems is how they vary in accomplishing
these tasks in a wide variety of computing environments. Operating systems are everywhere,
from cars and home appliances that include “Internet of Things” devices, to smart phones,
phones, personal computers, enterprise computers, and cloud computing environments.

The user’s view of the computer varies according to the interface being used.  Many computer
users sit with a laptop or in front of a PC consisting of a monitor, keyboard, and mouse.
Such a system is designed for one user to monopolize its resources. The goal is to maximize
the work (or play) that the user is performing. In this case, the operating system is
designed mostly for ease of use, with some attention paid to performance and security and
none paid to resource utilization—how various hardware and software resources are shared.

#+BEGIN_PREVIEW
Although there are many practitioners of computer science, only a small percentage of them
will be involved in the creation or modification of an operating system. Why, then, study
operating systems and how they work? Simply because, as almost all code runs on top of an
operating system, knowledge of how operating systems work is crucial to proper, efficient,
effective, and secure programming. Understanding the fundamentals of operating systems, how
they drive computer hardware, and what they provide to applications is not only essential to
those who program them but also highly useful to those who write programs on them and use
them.
#+END_PREVIEW

A modern general-purpose computer system consists of one or more CPUs and
a number of device controllers connected through a common bus that provides
access between components and shared memory (Figure 1.2). Each device
controller is in charge of a specific type of device (for example, a disk drive,
audio device, or graphics display). Depending on the controller, more than one
device may be attached. For instance, one system USB port can connect to a
USB hub, to which several devices can connect. A device controller maintains
some local buffer storage and a set of special-purpose registers. The device
controller is responsible for moving the data between the peripheral devices
that it controls and its local buffer storage.

* Computer-System Organization :chapter_1:

Consider a typical computer operation: a program performing I/O. To start an
I/O operation, the device driver loads the appropriate registers in the device
controller. The device controller, in turn, examines the contents of these reg-
isters to determine what action to take (such as “read a character from the
keyboard”). The controller starts the transfer of data from the device to its local
buffer. Once the transfer of data is complete, the device controller informs the
device driver that it has finished its operation. The device driver then gives
control to other parts of the operating system, possibly returning the data or a
pointer to the data if the operation was a read. For other operations, the device
driver returns status information such as “write completed successfully” or
“device busy”. But how does the controller inform the device driver that it has
finished its operation? This is accomplished via an interrupt.

** Interrupts

Typically, operating systems have a device driver for each device controller. This device
driver understands the device controller and provides the rest of the operating system with
a uniform interface to the device. The CPU and the device controllers can execute in
parallel, competing for memory cycles. To ensure orderly access to the shared memory, a
memory controller synchronizes access to the memory.

When the CPU is interrupted, it stops what it is doing and immediately _transfers execution
to a fixed location_. The fixed location usually contains the starting address where the
service routine for the interrupt is located.  The interrupt service routine executes; on
completion, the CPU resumes the interrupted computation.

Interrupts are an important part of a computer architecture. Each computer design has its
own interrupt mechanism, but several functions are common.  The interrupt must transfer
control to the appropriate /interrupt service routine[fn:1]/.  The straightforward method for
managing this transfer would be to invoke a generic routine to examine the interrupt
information. The routine, in turn, would call the interrupt-specific handler. However,
interrupts must be handled quickly, as they occur very frequently. *A table* of pointers to
interrupt routines can be used instead to provide the necessary speed. The interrupt routine
is called indirectly through the table, with no intermediate routine needed.  Generally, the
table of pointers is stored in low memory (*the first hundred* or so locations). These
locations hold the addresses of the interrupt service routines for the various devices. This
array, or *interrupt vector*, of addresses is then indexed by a unique number, given with
the interrupt request, to provide the address of the interrupt service routine for the
interrupting device. Operating systems as different as Windows and UNIX dispatch interrupts
in this manner.

The interrupt architecture must also save the state information of whatever was interrupted,
so that it can restore this information after servicing the interrupt. If the interrupt
routine needs to modify the processor state —for instance, by modifying register values—it
must explicitly save the current state and then restore that state before returning. After
the interrupt is serviced, the saved return address is loaded into the program counter, and
the interrupted computation resumes as though the interrupt had not occurred.

The basic interrupt mechanism works as follows:

+ The CPU hardware has a wire called the *interrupt-request line* that the CPU _senses after executing every instruction_.
+ When the CPU detects that a controller has asserted a signal on the interrupt-request line, it reads the interrupt number and jumps to the interrupt-handler routine by using that interrupt number as an index into the interrupt vector.
+ It then starts execution at the address associated with that index. The interrupt handler saves any state it will be changing during its operation, determines the cause of the interrupt, performs the necessary processing, performs a state restore, and executes a return from interrupt instruction to return the CPU to the execution state prior to the interrupt.

We say that the device controller raises an interrupt by asserting a signal on the interrupt
request line, the CPU catches the interrupt and dispatches it to the interrupt handler, and
the handler clears the interrupt by servicing the device. The following figure summarizes
the cycle:

#+DOWNLOADED: screenshot @ 2022-08-17 09:45:44
[[file:Computer-System_Organization/2022-08-17_09-45-44_screenshot.png]]

The CPU can load instructions only from memory, so any programs must first be loaded into
memory to run. General-purpose computers run most of their programs from rewritable memory,
called main memory (also called random-access memory, or RAM). Main memory commonly is
implemented in a semiconductor technology called dynamic random-access memory (DRAM).

Computers use other forms of memory as well. For example, the first pro- gram to run on
computer power-on is a bootstrap program, which then loads the operating system. Since RAM
is volatile—loses its content when power is turned off or otherwise lost—we cannot trust it
to hold the bootstrap pro- gram. Instead, for this and some other purposes, the computer
uses electrically erasable programmable read-only memory (EEPROM) and other forms of
firmware storage that is infrequently written to and is nonvolatile. EEPROM can be changed
but cannot be changed frequently. In addition, it is low speed, and so it contains mostly
static programs and data that aren’t frequently used.  For example, the iPhone uses EEPROM
to store serial numbers and hardware information about the device.

All forms of memory provide an array of bytes. Each byte has its own address. Interaction is
achieved through a sequence of load or store instructions to specific memory addresses.
The load instruction moves a byte or word from main memory to an internal register within
the CPU, whereas the store instruction moves the content of a register to main memory. Aside
from explicit loads and stores, the CPU automatically loads instructions from main memory
for execution from the location stored in the program counter.

** TODO  Processing :introductive:
** DONE  Multiprocessor System :introductive:
On modern computers, from mobile devices to servers, multiprocessor systems now dominate
the landscape of computing. Traditionally, such systems have two (or more) processors, each
with a single-core CPU. The processors share the computer bus and sometimes the clock,
memory, and peripheral devices. The primary advantage of multiprocessor systems is
increased throughput. That is, by increasing the number of processors, we expect to get more
work done in less time. The speed-up ratio with N processors is not N, however; it is less
than N. When multiple processors cooperate on a task, a certain amount of overhead is
incurred in keeping all the parts working correctly.  This overhead, plus contention for
shared resources, lowers the expected gain from additional processors.
* TODO System Calls :chapter_2:
* Why Applications Are Operating-System-Specific :chapter_2:
Why Applications Are Operating-System Specific Fundamentally, applications compiled on one
operating system are not executable on other operating systems. If they were, the world
would be a better place, and our choice of what operating system to use would depend on
utility and features rather than which applications were available.

Based on our earlier discussion, we can now see part of the problem—each operating system
provides a unique set of system calls. System calls are part of the set of services provided
by operating systems for use by applications. Even if system calls were somehow uniform,
other barriers would make it difficult for us to execute application programs on different
operating systems. But if you have used multiple operating systems, you may have used some
of the same applications on them. How is that possible?  An application can be made
available to run on multiple operating systems in one of three ways:

1. The application can be written in an interpreted language (such as Python
or Ruby) that has an interpreter available for multiple operating systems.  The interpreter
reads each line of the source program, executes equivalent instructions on the native
instruction set, and calls native operating sys- tem calls. Performance suffers relative to
that for native applications, and the interpreter provides only a subset of each operating
system’s features, possibly limiting the feature sets of the associated applications.



2. The application can be written in a language that includes a virtual
machine containing the running application. The virtual machine is part of the language’s
full RTE. One example of this method is Java. Java has an RTE that includes a loader,
byte-code verifier, and other components that load the Java application into the Java
virtual machine. This RTE has been ported, or developed, for many operating systems, from
mainframes to smartphones, and in theory any Java app can run within the RTE wherever it is
available. Systems of this kind have disadvantages similar to those of interpreters,
discussed above.



3. The application developer can use a standard language or API in which
the compiler generates binaries in a machine- and operating-system- specific language. The
application must be ported to each operating sys- tem on which it will run. This porting can
be quite time consuming and must be done for each new version of the application, with
subsequent testing and debugging. Perhaps the best-known example is the POSIX API and its
set of standards for maintaining source-code compatibility between different variants of
UNIX-like operating systems.

* Operating-System Design and Implementation :chapter_2:

The first problem in designing a system is to define goals and specifications. At the
highest level, the design of the system will be affected by the choice of hard- ware and the
type of system: traditional desktop/laptop, mobile, distributed, or real time[fn:2].


Beyond this highest design level, the requirements may be much harder to specify. The
requirements can, however, be divided into two basic groups: _user goals_ and _system goals_.

User[fn:3]s want certain obvious properties in a system. The system should be convenient to use,
easy to learn and to use, reliable, safe, and fast. Of course, these specifications are not
particularly useful in the system design, since there is no general agreement on how to
achieve them.


** Mechanisms and Policies :introductive:
*Mechanisms determine how to do something*; policies determine _what will be done_.  For
example, the timer construct is a mechanism for ensuring CPU protection, but deciding how
long the timer is to be set for a particular user is a policy decision.

The separation of policy and mechanism is important for flexibility. Policies are likely to
change across places or over time. In the worst case, each change in policy would require a
change in the underlying mechanism. A general mechanism flexible enough to work across a
range of policies is preferable.  A change in policy would then require redefinition of only
certain parameters of the system. For instance, consider a mechanism for giving priority to
certain types of programs over others. If the mechanism is properly separated from policy,
it can be used either to support a policy decision that I/O-intensive programs should have
priority over CPU-intensive ones or to support the opposite policy.


Microkernel-based operating systems (will be discussed later) take the separation of
mechanism and policy to one extreme by implementing a basic set of primitive building
blocks. These blocks are almost policy free, allowing more advanced mechanisms and policies
to be added via user-created kernel modules or user programs themselves. In contrast,
consider Windows, an enormously popular commercial operating system available for over three
decades. Microsoft has closely encoded both mechanism and policy into the system to enforce
a global look and feel across all devices that run the Windows operating system. All
applications have similar interfaces, because the interface itself is built into the kernel
and system libraries. Apple has adopted a similar strategy with its macOS and iOS operating
systems.

We can make a similar comparison between commercial and open-source operating systems. For
instance, contrast Windows, discussed above, with Linux, an open-source operating system
that runs on a wide range of computing devices and has been available for over 25 years.
The “standard” Linux kernel has a specific CPU scheduling algorithm, which is a mechanism
that supports a certain policy. However, anyone is free to modify or replace the scheduler
to support a different policy.



* Footnotes

[fn:3] Such needs might be ignored in some specifications, for example designing an os that
would be used for embedded systems purpose.
[fn:2] Implies the need of thinking of the resources available before implementing. ofc
android devices will enjoy less battery usage, such a thing should be considered during
designing the os.

[fn:1] An important note is that whenever an interrupt is generated, control is given for
the kernel. The kernel is responsible for sending the request for the corresponding I/O
device (from ISR table, so-called /interrupt-vector/).
