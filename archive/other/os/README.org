#+TITLE: Introduction into Operating Systems
#+DATE:  <2022-08-01>


#+begin_abstract
notes and exercises from OPERATING-SYSTEM CONCEPTS
#+end_abstract

An operating system is software that manages a computer’s hardware. It also provides a basis
for application programs and acts as an intermediary between the computer user and the
computer hardware. An amazing aspect of operating systems is how they vary in accomplishing
these tasks in a wide variety of computing environments. Operating systems are everywhere,
from cars and home appliances that include “Internet of Things” devices, to smart phones,
phones, personal computers, enterprise computers, and cloud computing environments.

The user’s view of the computer varies according to the interface being used.  Many computer
users sit with a laptop or in front of a PC consisting of a monitor, keyboard, and mouse.
Such a system is designed for one user to monopolize its resources. The goal is to maximize
the work (or play) that the user is performing. In this case, the operating system is
designed mostly for ease of use, with some attention paid to performance and security and
none paid to resource utilization—how various hardware and software resources are shared.

#+BEGIN_PREVIEW
Although there are many practitioners of computer science, only a small percentage of them
will be involved in the creation or modification of an operating system. Why, then, study
operating systems and how they work? Simply because, as almost all code runs on top of an
operating system, knowledge of how operating systems work is crucial to proper, efficient,
effective, and secure programming. Understanding the fundamentals of operating systems, how
they drive computer hardware, and what they provide to applications is not only essential to
those who program them but also highly useful to those who write programs on them and use
them.
#+END_PREVIEW

A modern general-purpose computer system consists of one or more CPUs and a number of device
controllers connected through a common bus that provides access between components and
shared memory (Figure 1.2). Each device controller is in charge of a specific type of device
(for example, a disk drive, audio device, or graphics display). Depending on the controller,
more than one device may be attached. For instance, one system USB port can connect to a USB
hub, to which several devices can connect. A device controller maintains some local buffer
storage and a set of special-purpose registers. The device controller is responsible for
moving the data between the peripheral devices that it controls and its local buffer
storage.

* Computer-System Organization :chapter_1:

Consider a typical computer operation: a program performing I/O. To start an
I/O operation, the device driver loads the appropriate registers in the device
controller. The device controller, in turn, examines the contents of these reg-
isters to determine what action to take (such as “read a character from the
keyboard”). The controller starts the transfer of data from the device to its local
buffer. Once the transfer of data is complete, the device controller informs the
device driver that it has finished its operation. The device driver then gives
control to other parts of the operating system, possibly returning the data or a
pointer to the data if the operation was a read. For other operations, the device
driver returns status information such as “write completed successfully” or
“device busy”. But how does the controller inform the device driver that it has
finished its operation? This is accomplished via an interrupt.

** Interrupts

Typically, operating systems have a device driver for each device controller. This device
driver understands the device controller and provides the rest of the operating system with
a uniform interface to the device. The CPU and the device controllers can execute in
parallel, competing for memory cycles. To ensure orderly access to the shared memory, a
memory controller synchronizes access to the memory.

When the CPU is interrupted, it stops what it is doing and immediately _transfers execution
to a fixed location_. The fixed location usually contains the starting address where the
service routine for the interrupt is located.  The interrupt service routine executes; on
completion, the CPU resumes the interrupted computation.

Interrupts are an important part of a computer architecture. Each computer design has its
own interrupt mechanism, but several functions are common.  The interrupt must transfer
control to the appropriate /interrupt service routine[fn:1]/.  The straightforward method for
managing this transfer would be to invoke a generic routine to examine the interrupt
information. The routine, in turn, would call the interrupt-specific handler. However,
interrupts must be handled quickly, as they occur very frequently. *A table* of pointers to
interrupt routines can be used instead to provide the necessary speed. The interrupt routine
is called indirectly through the table, with no intermediate routine needed.  Generally, the
table of pointers is stored in low memory (*the first hundred* or so locations). These
locations hold the addresses of the interrupt service routines for the various devices. This
array, or *interrupt vector*, of addresses is then indexed by a unique number, given with
the interrupt request, to provide the address of the interrupt service routine for the
interrupting device. Operating systems as different as Windows and UNIX dispatch interrupts
in this manner.

The interrupt architecture must also save the state information of whatever was interrupted,
so that it can restore this information after servicing the interrupt. If the interrupt
routine needs to modify the processor state —for instance, by modifying register values—it
must explicitly save the current state and then restore that state before returning. After
the interrupt is serviced, the saved return address is loaded into the program counter, and
the interrupted computation resumes as though the interrupt had not occurred.

The basic interrupt mechanism works as follows:

+ The CPU hardware has a wire called the *interrupt-request line* that the CPU _senses after executing every instruction_.
+ When the CPU detects that a controller has asserted a signal on the interrupt-request line, it reads the interrupt number and jumps to the interrupt-handler routine by using that interrupt number as an index into the interrupt vector.
+ It then starts execution at the address associated with that index. The interrupt handler saves any state it will be changing during its operation, determines the cause of the interrupt, performs the necessary processing, performs a state restore, and executes a return from interrupt instruction to return the CPU to the execution state prior to the interrupt.

We say that the device controller raises an interrupt by asserting a signal on the interrupt
request line, the CPU catches the interrupt and dispatches it to the interrupt handler, and
the handler clears the interrupt by servicing the device. The following figure summarizes
the cycle:

#+DOWNLOADED: screenshot @ 2022-08-17 09:45:44
[[file:Computer-System_Organization/2022-08-17_09-45-44_screenshot.png]]

The CPU can load instructions only from memory, so any programs must first be loaded into
memory to run. General-purpose computers run most of their programs from rewritable memory,
called main memory (also called random-access memory, or RAM). Main memory commonly is
implemented in a semiconductor technology called dynamic random-access memory (DRAM).

Computers use other forms of memory as well. For example, the first pro- gram to run on
computer power-on is a bootstrap program, which then loads the operating system. Since RAM
is volatile—loses its content when power is turned off or otherwise lost—we cannot trust it
to hold the bootstrap pro- gram. Instead, for this and some other purposes, the computer
uses electrically erasable programmable read-only memory (EEPROM) and other forms of
firmware storage that is infrequently written to and is nonvolatile. EEPROM can be changed
but cannot be changed frequently. In addition, it is low speed, and so it contains mostly
static programs and data that aren’t frequently used.  For example, the iPhone uses EEPROM
to store serial numbers and hardware information about the device.

All forms of memory provide an array of bytes. Each byte has its own address. Interaction is
achieved through a sequence of load or store instructions to specific memory addresses.
The load instruction moves a byte or word from main memory to an internal register within
the CPU, whereas the store instruction moves the content of a register to main memory. Aside
from explicit loads and stores, the CPU automatically loads instructions from main memory
for execution from the location stored in the program counter.

** TODO  Processing :introductive:
** DONE  Multiprocessor System :introductive:
On modern computers, from mobile devices to servers, multiprocessor systems now dominate
the landscape of computing. Traditionally, such systems have two (or more) processors, each
with a single-core CPU. The processors share the computer bus and sometimes the clock,
memory, and peripheral devices. The primary advantage of multiprocessor systems is
increased throughput. That is, by increasing the number of processors, we expect to get more
work done in less time. The speed-up ratio with N processors is not N, however; it is less
than N. When multiple processors cooperate on a task, a certain amount of overhead is
incurred in keeping all the parts working correctly.  This overhead, plus contention for
shared resources, lowers the expected gain from additional processors.
* TODO System Calls :chapter_2:
* Why Applications are Operating-System-Specific :chapter_2:
Why Applications Are Operating-System Specific Fundamentally, applications compiled on one
operating system are not executable on other operating systems. If they were, the world
would be a better place, and our choice of what operating system to use would depend on
utility and features rather than which applications were available.

Based on our earlier discussion, we can now see part of the problem—each operating system
provides a unique set of system calls. System calls are part of the set of services provided
by operating systems for use by applications. Even if system calls were somehow uniform,
other barriers would make it difficult for us to execute application programs on different
operating systems. But if you have used multiple operating systems, you may have used some
of the same applications on them. How is that possible?  An application can be made
available to run on multiple operating systems in one of three ways:

1. The application can be written in an interpreted language (such as Python or Ruby) that has an interpreter available for multiple operating systems.  The interpreter reads each line of the source program, executes equivalent instructions on the native instruction set, and calls native operating sys- tem calls. Performance suffers relative to that for native applications, and the interpreter provides only a subset of each operating system’s features, possibly limiting the feature sets of the associated applications.



2. The application can be written in a language that includes a virtual machine containing the running application. The virtual machine is part of the language’s full RTE. One example of this method is Java. Java has an RTE that includes a loader, byte-code verifier, and other components that load the Java application into the Java virtual machine. This RTE has been ported, or developed, for many operating systems, from mainframes to smartphones, and in theory any Java app can run within the RTE wherever it is available. Systems of this kind have disadvantages similar to those of interpreters, discussed above.



3. The application developer can use a standard language or API in which the compiler generates binaries in a machine- and operating-system- specific language. The application must be ported to each operating sys- tem on which it will run. This porting can be quite time consuming and must be done for each new version of the application, with subsequent testing and debugging. Perhaps the best-known example is the POSIX API and its set of standards for maintaining source-code compatibility between different variants of UNIX-like operating systems.

* Operating-System Design And Implementation :chapter_2:

The first problem in designing a system is to define goals and specifications. At the
highest level, the design of the system will be affected by the choice of hard- ware and the
type of system: traditional desktop/laptop, mobile, distributed, or real time[fn:2].


Beyond this highest design level, the requirements may be much harder to specify. The
requirements can, however, be divided into two basic groups: _user goals_ and _system goals_.

User[fn:3]s want certain obvious properties in a system. The system should be convenient to use,
easy to learn and to use, reliable, safe, and fast. Of course, these specifications are not
particularly useful in the system design, since there is no general agreement on how to
achieve them.


** Mechanisms and Policies :introductive:
*Mechanisms determine how to do something*; policies determine _what will be done_.  For
example, the timer construct is a mechanism for ensuring CPU protection, but deciding how
long the timer is to be set for a particular user is a policy decision.

The separation of policy and mechanism is important for flexibility. Policies are likely to
change across places or over time. In the worst case, each change in policy would require a
change in the underlying mechanism. A general mechanism flexible enough to work across a
range of policies is preferable.  A change in policy would then require redefinition of only
certain parameters of the system. For instance, consider a mechanism for giving priority to
certain types of programs over others. If the mechanism is properly separated from policy,
it can be used either to support a policy decision that I/O-intensive programs should have
priority over CPU-intensive ones or to support the opposite policy.


Microkernel-based operating systems (will be discussed later) take the separation of
mechanism and policy to one extreme by implementing a basic set of primitive building
blocks. These blocks are almost policy free, allowing more advanced mechanisms and policies
to be added via user-created kernel modules or user programs themselves. In contrast,
consider Windows, an enormously popular commercial operating system available for over three
decades. Microsoft has closely encoded both mechanism and policy into the system to enforce
a global look and feel across all devices that run the Windows operating system. All
applications have similar interfaces, because the interface itself is built into the kernel
and system libraries. Apple has adopted a similar strategy with its macOS and iOS operating
systems.

We can make a similar comparison between commercial and open-source operating systems. For
instance, contrast Windows, discussed above, with Linux, an open-source operating system
that runs on a wide range of computing devices and has been available for over 25 years.
The “standard” Linux kernel has a specific CPU scheduling algorithm, which is a mechanism
that supports a certain policy. However, anyone is free to modify or replace the scheduler
to support a different policy.

* Operating-System Structure :chapter_2:

A system as large and complex as a modern operating system must be engineered carefully if
it is to function properly and be modified easily. A common approach is to partition the
task into small components, or modules, rather than have one single system. Each of these
modules should be a well-defined portion of the system, with carefully defined interfaces
and functions. You may use a similar approach when you structure your programs: rather than
placing all of your code in the ~main()~ function, you instead separate logic into a number
of functions, clearly articulate parameters and return values, and then call those functions
from ~main()~.

** Monolithic Structure[fn:4]

#+DOWNLOADED: screenshot @ 2022-08-20 07:15:03
[[file:Operating-System_Structure/2022-08-20_07-15-03_screenshot.png]]


The simplest structure for organizing an operating system is no structure at all.  That is,
place all of the functionality of the kernel into a single, static binary file that runs in
a single address space. This approach—known as a monolithic structure—is a common technique
for designing operating systems.


An example of such limited structuring is the original UNIX operating system, which consists
of two separable parts: the kernel and the system programs. The kernel is further separated
into a series of interfaces and device drivers, which have been added and expanded over the
years as UNIX has evolved. We can view the traditional UNIX operating system as being
layered to some extent, as shown in Figure 2.12. Everything below the system-call interface
and above the physical hardware is the kernel. The kernel provides the file system, CPU
scheduling, memory management, and other operating- system functions through system calls.
Taken in sum, that is an enormous amount of functionality to be combined into one single
address space.

The Linux operating system is based on UNIX and is structured similarly, as shown:


#+DOWNLOADED: screenshot @ 2022-08-20 07:18:02
[[file:Operating-System_Structure/2022-08-20_07-18-02_screenshot.png]]


Applications typically use the glibc standard C library when communicating with the system
call interface to the kernel. The Linux kernel is ~monolithic~ in that it runs entirely in
kernel mode in a single address space, but as we shall, it does have a modular design that
allows the kernel to be modified during run time.  Despite the apparent simplicity of
monolithic kernels, they are difficult to implement and extend. Monolithic kernels do have a
distinct performance advantage, however: there is very little overhead in the system-call
interface, and communication within the kernel is fast. Therefore, despite the drawbacks of
monolithic kernels, their speed and efficiency explains why we still see evidence of this
structure in the UNIX, Linux, and Windows operating systems.
** Layered Approach

The monolithic approach is often known as a tightly coupled system because changes to one
part of the system can have wide-ranging effects on other parts.  Alternatively, we could
design *a loosely coupled system*. Such a system is divided into separate, smaller components
that have specific and limited functionality. All these components together comprise the
kernel. The advantage of this modular approach is that changes in one component affect only
that component, and no others, allowing system implementers more freedom in creating and
changing the inner workings of the system.

A system can be made modular in many ways. One method is the layered approach, in which the
operating system is broken into a number of layers (levels). The bottom layer (layer 0) is
the hardware; the highest (layer N[fn:5]) is the user interface. This layering structure is
depicted in Figure 2.14.

An operating-system layer is an implementation of an abstract object made up of data and the
operations that can manipulate those data. A typical operating-system layer—say, layer
M—consists of data structures and a set of functions that can be invoked by higher-level
layers. Layer M, in turn, can invoke operations on lower-level layers.

The main advantage of the layered approach is simplicity of construction
and debugging. The layers are selected so that each uses functions (operations)
and services of only lower-level layers. This approach simplifies debugging
and system verification. The first layer can be debugged without any concern
for the rest of the system, because, by definition, it uses only the basic hardware
(which is assumed correct) to implement its functions. Once the first layer is
debugged, its correct functioning can be assumed while the second layer is
debugged, and so on. If an error is found during the debugging of a particular
layer, the error must be on that layer, because the layers below it are already
debugged. Thus, the design and implementation of the system are simplified.


#+DOWNLOADED: screenshot @ 2022-08-20 07:47:49
[[file:Operating-System_Structure/2022-08-20_07-47-49_screenshot.png]]
** Microkernels
As UNIX expanded, the kernel became large and difficult to manage.  In the mid-1980s,
researchers at Carnegie Mellon University developed an operating system called Mach that
modularized the kernel using the *micro-kernel* approach. This method structures the
operating system by removing all nonessential components from the kernel and implementing
them as user- level programs that reside in separate address spaces. The result is a smaller
kernel. There is little consensus regarding which services should remain in the kernel and
which should be implemented in user space[fn:6]. Typically, however, microkernels provide minimal
process and memory management, in addition to a communication facility. Figure 2.15
illustrates the architecture of a typical microkernel.


#+DOWNLOADED: screenshot @ 2022-08-20 07:56:00
[[file:Operating-System_Structure/2022-08-20_07-56-00_screenshot.png]]


One benefit of the microkernel approach is that it makes extending the operating system
easier. All new services are added to user space and consequently do not require
modification of the kernel. When the kernel does have to be modified, the changes tend to be
fewer, because the microkernel is a smaller kernel. The resulting operating system is easier
to port from one hardware design to another. The microkernel also provides more security and
reliability, since most services are running as user—rather than kernel—processes. If a
service fails, the rest of the operating system remains untouched.

** Hybrid Systems
In practice, very few operating systems adopt a single, strictly defined struc-
ture. Instead, they combine different structures, resulting in hybrid systems
that address performance, security, and usability issues. For example, Linux
is monolithic, because having the operating system in a single address space
provides very efficient performance. However, it also modular, so that ne           w
functionality can be dynamically added to the kernel. Windows is largely
monolithic as well (again primarily for performance reasons), but it retains
some behavior typical of microkernel systems, including providing support
for separate subsystems (known as operating-system personalities) that run as
user-mode processes. Windows systems also provide support for dynamically
loadable kernel modules.

*** macOS and iOS

Apple’s macOS operating system is designed to run primarily on desktop and
laptop computer systems, whereas iOS is a mobile operating system designed
for the iPhone smartphone and iPad tablet computer. Architecturally, macOS
and iOS have much in common, and so we present them together, highlighting
what they share as well as how they differ from each other.


- *User experience layer*. This layer defines the software interface that allows users to interact with the computing devices. macOS uses the Aqua user interface, which is designed for a mouse or trackpad, whereas iOS uses the Springboard user interface, which is designed for touch devices.

- *Application frameworks layer*. This layer includes the Cocoa and Cocoa Touch frameworks, which provide an API for the Objective-C and Swift programming languages. The primary difference between Cocoa and Cocoa Touch is that the former is used for developing macOS applications, and the latter by iOS to provide support for hardware features unique to mobile devices, such as touch screens.
- *Core frameworks*. This layer defines frameworks that support graphics and media including Quicktime and OpenGL.

- *Kernel environment*. This environment, also known as Darwin, includes the Mach microkernel and the BSD UNIX kernel. We will elaborate on Darwin shortly.


**** TODO Android

* TODO Introduction To Linux Kernel Modules :programming_proeject:chapter_2:

-----

/TIP!/

/Kernel modules are loaded using the insmod command, which is run as follows:/
#+begin_src shell
sudo insmod model.ko
#+end_src

/To check whether the module has loaded, enter the lsmod command and search
for the module simple. Recall that the module entry point is invoked when the
module is inserted into the kernel. To check the contents of this message in the
kernel log buffer, enter the command/

/Removing the kernel module involves invoking the rmmod command (notice that the .ko suffix
is unnecessary)/

#+begin_src shell
sudo rmmode model
#+end_src

/and use ~dmesg~ to read your outputs from the kernel's log buffer (if any)./

-----

The first part of this project involves following a series of steps for creating and
inserting a module into the Linux kernel.  You can list all kernel modules that are
currently loaded by entering the command
#+begin_src shell
lsmod
#+end_src

This command will list the current kernel modules in three columns: name,
size, and where the module is being used.

Now, observe the following file[fn:7]:

#+begin_src csharp
#include <linux/init.h>
#include <linux/kernel.h>
#include <linux/module.h>
/* This function is called when the module is loaded. */
int simple init(void)
{
printk(KERN INFO "Loading Kernel Module∖n");
}
return 0;
/* This function is called when the module is removed. */
void simple exit(void)
{
printk(KERN INFO "Removing Kernel Module∖n");
}
/* Macros for registering module entry and exit points. */
module init(simple init);
module exit(simple exit);
MODULE LICENSE("GPL");
MODULE DESCRIPTION("Simple Module");
MODULE AUTHOR("SGG");
#+end_src

The function simple ~init()~ is the module entry point, which represents the function that is
invoked when the module is loaded into the kernel. Similarly, the simple ~exit()~ function
is the module exit point—the function that is called when the module is removed from the
kernel.

Notice in the figure how the module entry and exit point functions make calls to the
~printk()~ function. ~printk()~ is the kernel equivalent of ~printf()~, but its output is sent to
a kernel log buffer whose contents can be read by the dmesg command. One difference between
~printf()~ and ~printk()~ is that ~printk()~ allows us to specify a priority flag, whose values
are given in the ~<linux/printk.h>~ include file. In this instance, the priority is ~KERN INFO~,
which is defined as an informational message.

As kernel modules are running within the kernel, it is possible to obtain
values and call functions that are available only in the kernel and not to regular
user applications. For example, the Linux include file ~<linux/hash.h>~ defines
several hashing functions for use within the kernel. This file also defines the
constant value ~GOLDEN_RATIO_PRIME~ (which is defined as an unsigned long).
This value can be printed out as follows:

#+begin_src C
#include <linux/printk.h>
int main() {
    printk(KERN INFO "%lu∖n", GOLDEN_RATIO_PRIME);
}
#+end_src

As another example, the include file ~<linux/gcd.h>~ defines the following function

#+begin_src C
unsigned long gcd(unsigned long a, unsigned b);
#+end_src

which returns the greatest common divisor of the parameters $a$ and $b$.

What we have to do, is:

1. Print out the value of ~GOLDEN_RATIO_PRIME~ in the ~simple_init()~ function.
   #+begin_src C
static int simple_init(void) {
  printk(KERN_INFO "Loading Module\n");
  printk(GOLDEN_RATIO_PRIME) return 0;
}
   #+end_src
2. Print out the greatest common divisor of 3,300 and 24 in the ~simple_exit()~ function.
   #+begin_src C
#include <linux/gcd.h>
static void simple_exit(void) {
  printk(KERN_INFO "Removing Module\n");
  printk(gcd(3300, 24));
}
   #+end_src

ntdlr. solution: [[https://github.com/athultr1997/OS/blob/master/os_concepts_abraham_silberschatz/chapter_2/programming_projects/linux_kernel_modules/simple.c][here]]
* The Process :chapter_4:
The definition of a process, informally, is quite simple: it is a running program The
program itself is a lifeless thing: it just sits there on the disk, a bunch of instructions
(and maybe some static data), waiting to spring into action. It is the oper- ating system
that takes these bytes and gets them running, transforming the program into something
useful.

The OS creates this illusion by virtualizing the CPU. By running one process, then stopping
it and running another, and so forth, the OS can promote the illusion that many virtual CPUs
exist when in fact there is only one physical CPU (or a few). This basic technique, known as
*time sharing* of the CPU, allows users to run as many concurrent processes as they would
like; the potential cost is performance, as each will run more slowly if the CPU(s) must be
shared.

The abstraction provided by the OS of a running program is something we will call a process.
As we said above, a process is simply a running program; at any instant in time, we can
summarize a process by taking an inventory of the different pieces of the system it accesses
or affects during the course of its execution.

To understand what constitutes a process, we thus have to understand
its machine state: what a program can read or update when it is running.
At any given time, what parts of the machine are important to the execution of this program?

 + One obvious component of machine state that comprises a process is its memory. Instructions lie in memory; the data that the running pro- gram reads and writes sits in memory as well. Thus the memory that the process can address (called its address space) is part of the process
 + Also part of the process’s machine state are registers; many instructions explicitly read or update registers and thus clearly they are important to the execution of the process.

   Note that there are some particularly special registers that form part of this machine
   state. For example, the *program counter* (PC) (sometimes called the *instruction pointer* or
   IP) tells us which instruction of the program _is currently being executed_; similarly a
   *stack pointer* and associated *frame pointer* are used to manage the stack for *function
   parameters*, *local variables*, and *return addresses*.

 + *Programming Interface*
     The following APIs, in some form, are available on any modern operating system.

   + *Create*: An operating system must include some method to create new processes. When you type a command into the shell, or double-click on an application icon, the OS is invoked to create a new process to run the program you have indicated.

     One mystery that we should unmask a bit is how programs are trans- formed into
     processes. Specifically, how does the OS get a program up and running? How does
     process creation actually work?

     The first thing that the OS must do to run a program is to load its code and any
     static data (e.g., initialized variables) into memory, into the ad- dress space of the
     process. Programs initially reside on disk (or, in some modern systems, flash-based
     SSDs) in some kind of executable format; thus, the process of loading a program and
     static data into memory re- quires the OS to read those bytes from disk and place them
     in memory somewhere.

     In early (or simple) operating systems, the loading process is done eagerly, i.e.,
     all at once before running the program; modern OSes perform the process lazily, i.e.,
     by loading pieces of code or data only as they are needed during program execution.

     Once the code and static data are loaded into memory, there are a few other things the
     OS needs to do before running the process. Some memory must be allocated for the
     program’s run-time stack (or just stack). As you should likely already know, *C
     programs use the stack for local variables, function parameters, and return addresses*;
     the OS allocates this memory and gives it to the process. The OS will also likely
     initialize the stack with arguments; specifically, it will fill in the parameters to
     the main() function, i.e., ~argc~ and the ~argv~ array.

     The OS may also allocate some memory for the program’s heap. In C programs, the heap
     is used for explicitly requested /dynamically-allocated data/; programs request such
     space by calling *malloc()* and free it explicitly by calling *free()*. The heap is
     needed for data structures such as linked lists, hash tables, trees, and other
     interesting data structures. The heap will be small at first; as the program runs, and
     requests more mem- ory via the malloc() library API, the OS may get involved and
     allocate more memory to the process to help satisfy such calls.


    + *Destroy*: As there is an interface for process creation, systems also provide an interface
      to destroy processes forcefully. Of course, many processes will run and just exit by
      themselves when complete; when they don’t, however, the user may wish to kill them, and
      thus an interface to halt a runaway process is quite useful.

    + *Wait*: Sometimes it is useful to wait for a process to stop running; thus some kind of
      waiting interface is often provided.

    + *Miscellaneous Control*: Other than killing or waiting for a process, there are sometimes
      other controls that are possible. For example, most operating systems provide some kind of
      method to suspend a process (stop it from running for a while) and then resume it (continue it running).

    + *Status*: There are usually interfaces to get some status information about a process as
      well, such as how long it has run for, or what state it is in.

      In a simplified view, a process can be in one of three states:

      - *Running*: In the running state, a process is running on a processor. This means it is executing instructions.

      - *Ready*: In the ready state, a process is ready to run but for some reason the OS has chosen not to run it at this given moment.

      - *Blocked*: In the blocked state, a process has performed some kind of operation that makes it not ready to run until some other event takes place. A common example: when a process initiates an I/O request to a disk, it becomes blocked and thus some other process can use the processor.


Process can be described as either I/O bound or CPU bound. Briefly we can say that an I/O
process is a process that does I/O more than CPU, and vice versa.
** Data Structures

The OS is a program, and like any program, it has some key data structures that track
various relevant pieces of information. To track the state of each process, for example, the
OS likely will keep some kind of process list for all processes that are ready and some
additional information to track which process is currently running. The OS must also track,
in some way, blocked processes; when an I/O event completes, the OS should make sure to wake
the correct process and ready it to run again.

The following code shows what type of information an OS needs to track about
each process in the xv6 kernel. Similar process structures exist
in “real” operating systems such as Linux, Mac OS X, or Windows; look
them up and see how much more complex they are.


#+begin_src C
struct context {
  uint edi;
  uint esi;
  uint ebx;
  uint ebp;
  uint eip;
};

enum procstate { UNUSED, EMBRYO, SLEEPING, RUNNABLE, RUNNING, ZOMBIE };

// Per-process state
struct proc {
  uint sz;                     // Size of process memory (bytes)
  pde_t* pgdir;                // Page table
  char *kstack;                // Bottom of kernel stack for this process
  enum procstate state;        // Process state
  int pid;                     // Process ID
  struct proc *parent;         // Parent process
  struct trapframe *tf;        // Trap frame for current syscall
  struct context *context;     // swtch() here to run process
  void *chan;                  // If non-zero, sleeping on chan
  int killed;                  // If non-zero, have been killed
  struct file *ofile[NOFILE];  // Open files
  struct inode *cwd;           // Current directory
  char name[16];               // Process name (debugging)
};
#+end_src
** Shedulers

In operating systems there are different type of schedulers:
+ Short-term schedulers (the *CPU scheduler*) which selects which process should be excuted
  next for the CPU. And this is the one we are mainly intersted in.
  + sometimes it is the only scheduler in the system (like in UNIX systems).
  + invoked very frequently (in milliseconds or faster).
+ Long-term schedular, or the job schedular, selects which process should be load into the
  ready queue.

* Mechanism: Limited Directed Execution

What happen within a function call in a process? A function call translates to a jump
instruction i.e. the instructions points somewhere else, while the program counter points in
the consequential instruction. Before that happens, a stack frame is created with all
information of this program counter (that, it will be used to return again from this
function call). Then the CPU moves to the desired location from the stack frame, when it
does finish you now are allowed to pop the counter from the stack frame.


There are some differences between a system call and a function call, that's function calls
mostly executes within the user mode, some of the system calls also run within user mode but
lots of them run only within kernel mode. Another difference is that kernel does not trust
user stack, it uses a septate kernel stack when it is running in kernel mode.

One of the reasons that Kernel does not trust the user stack for, is that the user can
provide a wrong address (or a malicious one), so the kernel sets up *Interrupt Descriptor
Table (IDT)* at boot time, which has addresses of kernel function to run for a system call
and other events.

When a system calls accrues, the compiler inserts a special trap instruction (e.g. when you
try to read). First thing that happens within these instructions, is that it changes user mode into
kernel mode. Now, the stack pointer updates to point to the kernel stack, on the kernel
stack we save the old context (registers and etc.. just like function calls), then the trap
instructions looks up the IDT and jump to the desired function

In order to virtualize the CPU, the operating system needs to somehow share the physical CPU
among many jobs running seemingly at the same time. The basic idea is simple: run one
process for a little while, then run another one, and so forth. By time sharing the CPU in
this manner, virtualization is achieved.

There are a few challenges, however, in building such virtualization machinery. The first is
performance: how can we implement virtualization without adding excessive overhead to the
system? The second is control: how can we run processes efficiently while retaining control
over the CPU? Control is particularly important to the OS, as it is in charge of resources;
without control, a process could simply run forever and take over the machine, or access
information that it should not be allowed to access. Obtaining high performance while
maintaining control is thus one of the central challenges in building an operating system.

To make a program run as fast as one might expect, not surprisingly OS developers came up
with a technique, which we call limited direct execution. The “direct execution” part of the
idea is simple: just run the program directly on the CPU. Thus, when the OS wishes to start
a pro- gram running, it creates a process entry for it in a process list, allocates some
memory for it, loads the program code into memory (from disk), lo- cates its entry point
(i.e., the ~main()~ routine or something similar), jumps to it, and starts running the user’s
code.


Sounds simple, no? But this approach gives rise to a few problems in our quest to virtualize
the CPU. The first is simple: if we just run a program, how can the OS make sure the program
doesn’t do anything that we don’t want it to do, while still running it efficiently? The
second: when we are running a process, how does the operating system stop it from running
and switch to another process, thus implementing the time sharing we require to virtualize
the CPU?

** Problem #1: Restricted Operations
Direct execution has the obvious advantage of being fast; the program runs naively on the
hardware CPU and thus executes as quickly as one would expect. But running on the CPU
introduces a problem: what if the process wishes to perform some kind of restricted
operation, such as issuing an I/O request to a disk, or gaining access to more system
resources such as CPU or memory?

The approach we take is to introduce a new processor mode, known as user mode; code that
runs in user mode is restricted in what it can do. For example, when running in user mode, a
process can’t issue I/O requests; doing so would result in the processor raising an
exception; the OS would then likely kill the process.

In contrast to user mode is kernel mode, which the operating system (or kernel) runs in. In
this mode, code that runs can do what it likes, including privileged operations such as
issuing I/O requests and executing all types of restricted instructions.


We are still left with a challenge, however: what should a user process do when it wishes to
perform some kind of privileged operation, such as reading from disk? To enable this,
virtually all modern hard- ware provides the ability for user programs to perform a system
call.  Pioneered on ancient machines such as the Atlas, system calls allow the kernel to
carefully expose certain key pieces of functionality to user programs, such as accessing the
file system, creating and destroy- ing processes, communicating with other processes, and
allocating more memory. Most operating systems provide a few hundred calls.

To execute a system call, a program must execute a special trap instruction. This
instruction simultaneously jumps into the kernel and raises the privilege level to kernel
mode; once in the kernel, the system can now per- form whatever privileged operations are
needed (if allowed), and thus do the required work for the calling process. When finished,
the OS calls a special return-from-trap instruction, which, as you might expect, returns
into the calling user program while simultaneously reducing the privilege level back to user
mode.

** Problem #2: Switching Between Processes
The next problem with direct execution is achieving a switch between processes. Switching
between processes should be simple, right? The OS should just decide to stop one process and
start another. What’s the big deal? But it actually is a little bit tricky: specifically, if
a process is running on the CPU, this by definition means the OS is not running[fn:8] . If the OS
is not running, how can it do anything at all? (hint: it can’t) While this sounds almost
philosophical, it is a real problem: there is clearly no way for the OS to take an action if
it is not running on the CPU. Thus we arrive at the crux of the problem.


One approach that some systems have taken in the past (for example, early versions of the
Macintosh operating system, or the old Xerox Alto system) is known as the *cooperative
approach*. In this style, the OS trusts the /processes/ of the system to behave reasonably.
Processes that run for too long are assumed to periodically give up the CPU so that the OS
can decide to run some other task.

Thus, you might ask, how does a friendly process give up the CPU in this utopian world? Most
processes, as it turns out, transfer control of the CPU to the OS quite frequently by making
system calls, for example, to open a file and subsequently read it, or to send a message to
another machine, or to create a new process. Systems like this often include an explicit
yield system call, which does nothing except to transfer control to the OS so it can run
other processes.

Applications also transfer control to the OS when they do something illegal. For example, if
an application divides by zero, or tries to access memory that it shouldn’t be able to
access, it will generate a trap to the OS. The OS will then have control of the CPU again
(and likely terminate the offending process).


Thus, in a cooperative scheduling system, the OS regains control of the CPU by waiting for a
system call or an illegal operation of some kind to take place. You might also be thinking:
isn’t this passive approach less than ideal? What happens, for example, if a process
(whether malicious, or just full of bugs) ends up in an infinite loop, and never makes a
system call? What can the OS do then?

*The OS Takes Control*

Without some additional help from the hardware, it turns out the OS can’t do much at all
when a process refuses to make system calls (or mistakes) and thus return control to the OS.
In fact, in the cooperative approach, your only recourse when a process gets stuck in an
infinite loop is to resort to the age-old solution to all problems in computer systems:
reboot the machine. Thus, we again arrive at a subproblem of our general quest to gain
control of the CPU.

The answer turns out to be simple and was discovered by a number of people building computer
systems many years ago: a timer interrupt. A timer device can be programmed to raise an
interrupt every so many milliseconds; when the interrupt is raised, the currently running
process is halted, and a pre-configured interrupt handler in the OS runs.  At this point,
the OS has regained control of the CPU, and thus can do what it pleases: stop the current
process, and start a different one[fn:9] .

As we discussed before with system calls, the OS must inform the hardware of which code to
run when the timer interrupt occurs; thus, at boot time, the OS does exactly that. Second,
also during the boot sequence, the OS must start the timer, which is of course a privileged
operation. Once the timer has begun, the OS can thus feel safe in that control will
eventually be returned to it, and thus the OS is free to run user programs. The timer can
also be turned off (also a privileged operation), something we will discuss later when we
understand concurrency in more detail.

Note that the hardware has some responsibility when an interrupt occurs, in particular to
save enough of the state of the program that was running when the interrupt occurred such
that a subsequent return-from- trap instruction will be able to resume the running program
correctly.  This set of actions is quite similar to the behavior of the hardware during an
explicit system-call trap into the kernel, with various registers thus getting saved (e.g.,
onto a kernel stack) and thus easily restored by the return-from-trap instruction.

* The Multi-Level Feedback Queue

One of the most well-known approaches to scheduling is known as the Multi-level Feedback
Queue.  was first described by Corbato et al. in 1962 [C+62] in a system known as the
Compatible Time-Sharing System (CTSS), and this work, along with later work on Multics, led
the ACM to award Corbato its highest honor, the Turing Award. The scheduler has subsequently
been refined throughout the years to the implementations you will encounter in some modern
systems.

The fundamental problem MLFQ tries to address is two-fold. First, it would like to optimize
turnaround time, which, as we saw in the previous note, is done by running shorter jobs
first; unfortunately, the OS doesn’t generally know how long a job will run for, exactly the
knowledge that algorithms like SJF (or STCF) require. Second, MLFQ would like to make a
system feel responsive to interactive users (i.e., users sitting and staring at the screen,
waiting for a process to finish), and thus minimize response time; unfortunately, algorithms
like Round Robin reduce response time but are terrible for turnaround time. Thus, our
problem: given that we in general do not know anything about a process, how can we build a
scheduler to achieve these goals? How can the scheduler learn, as the system runs, the
characteristics of the jobs it is running, and thus make better scheduling decisions?

By now low-level mechanisms of running processes (e.g., context switching) should be clear;
if they are not, go back a note or two, and read the description of how that stuff works
again. However, we have yet to understand the high-level policies that an OS scheduler
employs. We will now do just that, presenting a series of scheduling policies (sometimes
called disciplines) that various smart and hard-working people have developed over the
years.


Each process have a parent, like a k-tree, the root/parent of all processes is the init
process. If we have a process, let's say a parent process that excutes ~fork()~, which
creates a process ~pid~ in UNIX, when the process is created, both the process and its
parent are running concurrently.

Running concurrently means that both are actng and computing for the CPU, let a system have
a single CPU

* TODO Lottery scheduling
* TODO Multi-CPU scheduling
* TODO Summary dialogue on cpu virtualization
* TODO A dialogue on memory virtualization
* TODO Address spaces
* TODO Memory API
* TODO Address translation
* TODO Segmentation
* TODO Free space management
* TODO Introduction to paging
* TODO Translation lookaside buffers
* TODO Advanced page tables
* TODO Swapping: mechanisms
* TODO Swapping: policies
* TODO Complete WM system
* TODO A dialogue on concurrency
* TODO Concurrency and threads
* TODO Thread API
* TODO Locks
* TODO Locked data structures
* TODO Condition variables
* TODO Semaphores
* TODO Concurrency bugs
* TODO Event-based concurrency
* TODO Summary dialogue on concurrency
* TODO Persistence
* TODO A Dialogue on Persistence
* TODO I/O Devices
* TODO Hard Disk Drives
* TODO Redundant Disk Arrays
* TODO Files And Directories
* TODO File System Implementation
* TODO Fast File System (FFS)
* TODO Fsck And Journaling
* TODO Log-Structured File System (LFS)
* TODO Flash-Based SSDs
* TODO Data Integrity And Protection
* TODO Summary Dialogue On Persistence
* TODO A Dialogue on Distribution
* TODO Distributed Systems
* TODO Network File System (NFS)
* TODO Andrew File System (AFS)
* TODO Summary Dialogue on Distribution




* Footnotes

[fn:9] Defines by the CPU scheduler
[fn:8] It is not running, but it is loaded in memoery

[fn:7] This file is provided inside the Operating System Concepts book source code.
[fn:6] This is a design decision problem.

[fn:5] In such a design, if you are in the layer $n$, you can only make calls for the $n-1$
layer, so you cannot talk to the layer above it nor any of the layers under the $n-1$ of
course, it can be gathered recursively from $n-1-k$, in which $k$ is the number of the layer
you are trying to reach, but here is a big performance issue since you have to stack many
calls just to go though the layers under-hood, this is very expensive for an OS which is to
be fast. Thus, the monolithic design wins in this comparison, however the layered one wins
when it comes to software engineering.
[fn:4] Of course notice, that all of OSs IRL are hybrid.

[fn:3] Such needs might be ignored in some specifications, for example designing an os that
would be used for embedded systems purpose.
[fn:2] Implies the need of thinking of the resources available before implementing. ofc
android devices will enjoy less battery usage, such a thing should be considered during
designing the os.

[fn:1] An important note is that whenever an interrupt is generated, control is given for
the kernel. The kernel is responsible for sending the request for the corresponding I/O
device (from ISR table, so-called /interrupt-vector/).


k

kp
kopkopk
